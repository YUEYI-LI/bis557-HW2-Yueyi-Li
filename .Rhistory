l2_errors[k] <- sqrt(sum((betahat - beta)^2))
}
mean(l2_errors)
max(svals) / min(svals)
lambda <- opt_ridge(X, y, 0:300)
# calculate ridge mse using optimized lambda
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
betahat <- ridge(X, y, lambda)
l2_errors[k] <- sqrt(sum((betahat - beta)^2))
}
mean(l2_errors)
### Case 2: lower numerical stability
# simulate a data set with colinearity
alpha <- 0.001
X[,1] <- X[,1] * alpha + X[,2] * (1 - alpha)
svals <- svd(X)$d
max(svals) / min(svals)
# calculate least squares estimates and the mean square error - statistical error
N <- 1e4
l2_errors <- rep(0, N)
ls_errors <- rep(0, N)
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
betahat <- casl_ols_svd(X, y)
ls_errors[k] <- sqrt(sum((betahat - beta)^2))
}
mean(ls_errors)  # 0.1584
# calculate the ridge estimates and the mean squared error
# calculate the optimized lambda
lambda <- opt_ridge(X, y, 0:100) # 27
# calculate ridge mse using optimized lambda
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
betahat <- ridge(X, y, lambda)
rg_errors[k] <- sqrt(sum((betahat - beta)^2))
}
rg_errors <- rep(0, N)
# calculate ridge mse using optimized lambda
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
betahat <- ridge(X, y, lambda)
rg_errors[k] <- sqrt(sum((betahat - beta)^2))
}
mean(rg_errors)  # 0.1564
install.packages("pandocfilters")
browseVignettes(lm)
browseVignettes('lm')
browseVignettes('ggplot2')
usethis::use_vignette('my-vignette')
library(bis557)
knitr::include_graphics("/Users/fanny/Downloads/WechatIMG179.JPEG")
knitr::include_graphics("/Users/fanny/Downloads/WechatIMG178.JPEG")
svals <- svd(t(X) %*% X)$d
max(svals) / min(svals) # 1.34 is relatively low
# measure of numerical stability - conditional number
svals <- svd(X)$d
max(svals) / min(svals) # 1.34 is relatively low
# simulate a data set
set.seed(123)
n <- 1000; p <- 25
beta <- c(1, rep(0, p - 1))
X <- matrix(rnorm(n * p), ncol = p)
# measure of numerical stability - conditional number
svals <- svd(X)$d
max(svals) / min(svals)
diag(3)
# measure of numerical stability - conditional number
# For OLS
svals <- svd(X)$d
max(svals) / min(svals) # 1.34 is relatively low
# For Ridge
svals <- svd(X + 27 * diag(nrow(X)))$d
max(svals) / min(svals)
X + 27 * diag(nrow(X))
27 * diag(nrow(X))
diag(3)
M<- 27 * diag(nrow(X))
View(M)
dim(x)
nrow(X)
M <- X + 27 * diag(nrow(X))
M <- diag(nrow(X))
M[2,]
M <- 27 * M
# measure of numerical stability - conditional number
# For OLS
svals <- svd(t(X) %*% X)$d
max(svals) / min(svals) # 1.34 is relatively low
# For Ridge
svals <- svd(t(X) %*% X + 27 * diag(nrow(X)))$d
dim(t(X) %*% X)
svals <- svd(X %*% t(X))$d
max(svals) / min(svals) # 1.34 is relatively low
dim(X %*% t(X))
dim(x)
dim(X)
# measure of numerical stability - conditional number
# For OLS
svals <- svd(t(X) %*% X)$d
max(svals) / min(svals) # 1.34 is relatively low
# For Ridge
svals <- svd(t(X) %*% X + 27 * diag(ncol(X)))$d
max(svals) / min(svals)
### Case 2: Colinear X
# simulate a data set with colinearity
alpha <- 0.001
# measure of numerical stability - conditional number
# For OLS
svals <- svd(t(X) %*% X)$d
### Case 2: Colinear X
# simulate a data set with colinearity
alpha <- 0.001
X[,1] <- X[,1] * alpha + X[,2] * (1 - alpha)
# measure of numerical stability - conditional number
# For OLS
svals <- svd(t(X) %*% X)$d
max(svals) / min(svals)
n <- 1000
p <- 25
beta <- c(1, rep(0, p - 1))
X <- matrix(rnorm(n * p), ncol = p)
########## simulation study
### Case 1: Normal X (no colinearity)
# simulate a data set
set.seed(123)
n <- 1000
p <- 25
beta <- c(1, rep(0, p - 1))
X <- matrix(rnorm(n * p), ncol = p)
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
betahat <- casl_ols_svd(X, y)
ls_errors[k] <- sqrt(sum((betahat - beta)^2))
}
mean(ls_errors)  # 0.1584
# calculate the ridge estimates and the mean squared error
# calculate the optimized lambda
lambda <- opt_ridge(X, y, 0:100) # 27
### Case 2: Colinear X
# simulate a data set with colinearity
alpha <- 0.001
X[,1] <- X[,1] * alpha + X[,2] * (1 - alpha)
# measure of numerical stability - conditional number
# For OLS
svals <- svd(t(X) %*% X)$d
max(svals) / min(svals)
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
betahat <- casl_ols_svd(X, y)
ls_errors[k] <- sqrt(sum((betahat - beta)^2))
}
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
betahat <- casl_ols_svd(X, y)
ls_errors[k] <- sqrt(sum((betahat - beta)^2))
}
mean(ls_errors)  # 36.27  inflate more than 200 times
# calculate the ridge estimates and the mean squared error
# calculate the optimized lambda
lambda <- opt_ridge(X, y, 0:100) # 27
# For Ridge
svals <- svd(t(X) %*% X + 94 * diag(ncol(X)))$d
max(svals) / min(svals)
# calculate least squares estimates and the mean square error - statistical error
N <- 1e4
ls_errors <- rep(0, N)
mean(ls_errors)  # 36.27  inflate more than 200 times
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
betahat <- casl_ols_svd(X, y)
ls_errors[k] <- sqrt(sum((betahat - beta)^2))
}
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
betahat <- casl_ols_svd(X, y)
ls_errors[k] <- sqrt(sum((betahat - beta)^2))
}
mean(ls_errors)  # 36.27  inflate more than 200 times
# calculate ridge mse using optimized lambda
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
betahat <- ridge(X, y, lambda)
rg_errors[k] <- sqrt(sum((betahat - beta)^2))
}
# calculate ridge mse using optimized lambda
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
betahat <- ridge(X, y, lambda)
rg_errors[k] <- sqrt(sum((betahat - beta)^2))
}
mean(rg_errors)  # 0.722
knitr::include_graphics("/Users/fanny/Downloads/WechatIMG179.JPEG")
# calculate the ridge estimates and the mean squared error
# calculate the optimized lambda
lambda <- opt_ridge(X, y, 0:100) # 27
########## simulation study
### Case 1: Normal X (no colinearity)
# simulate a data set
set.seed(123)
n <- 1000
p <- 25
beta <- c(1, rep(0, p - 1))
X <- matrix(rnorm(n * p), ncol = p)
# measure of numerical stability - conditional number
# For OLS
svals <- svd(t(X) %*% X)$d
max(svals) / min(svals)
# For Ridge
svals <- svd(t(X) %*% X + 27 * diag(ncol(X)))$d
max(svals) / min(svals)
# calculate least squares estimates and the mean square error - statistical error
N <- 1e4
ls_errors <- rep(0, N)
casl_ols_svd <-function(X, y){
svd_output <- svd(X)
r <- sum(svd_output$d > .Machine$double.eps)
U <- svd_output$u[, 1:r]
V <- svd_output$v[, 1:r]
beta <- V %*% (t(U) %*% y / svd_output$d[1:r])
beta
}
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
betahat <- casl_ols_svd(X, y)
ls_errors[k] <- sqrt(sum((betahat - beta)^2))
}
mean(ls_errors)  # 0.1584
# calculate the ridge estimates and the mean squared error
# calculate the optimized lambda
lambda <- opt_ridge(X, y, 0:100) # 27
rg_errors <- rep(0, N)
# calculate ridge mse using optimized lambda
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
betahat <- ridge(X, y, lambda)
rg_errors[k] <- sqrt(sum((betahat - beta)^2))
}
mean(rg_errors)  # 0.1564
### Case 2: Colinear X
# simulate a data set with colinearity
alpha <- 0.001
X[,1] <- X[,1] * alpha + X[,2] * (1 - alpha)
# measure of numerical stability - conditional number
# For OLS
svals <- svd(t(X) %*% X)$d
max(svals) / min(svals)  # 4391608
# For Ridge
svals <- svd(t(X) %*% X + 94 * diag(ncol(X)))$d
max(svals) / min(svals)  # 23.24
# calculate least squares estimates and the mean square error - statistical error
N <- 1e4
ls_errors <- rep(0, N)
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
betahat <- casl_ols_svd(X, y)
ls_errors[k] <- sqrt(sum((betahat - beta)^2))
}
mean(ls_errors)  # 36.27  inflate more than 200 times
# calculate the ridge estimates and the mean squared error
# calculate the optimized lambda
lambda <- opt_ridge(X, y, 0:100) # 94
# calculate ridge mse using optimized lambda
for (k in 1:N) {
y <- X %*% beta + rnorm(n)
betahat <- ridge(X, y, lambda)
rg_errors[k] <- sqrt(sum((betahat - beta)^2))
}
mean(rg_errors)  # 0.722
# ridge functions
ridge <- function(X, y, lambda_vals) {
# scale the independent variables
X <- scale(X)
svd_obj <- svd(X)
U <- svd_obj$u
V <- svd_obj$v
d <- svd_obj$d
ridge_beta <- matrix(NA_real_, nrow = length(lambda_vals), ncol = ncol(X))
for (i in 1 : length(lambda_vals)) {
D <- diag(d / (d^2 + lambda_vals[i]))
ridge_beta[i,] <- V %*% D %*% t(U) %*% y
}
ridge_beta
}
opt_ridge <- function (X, y, lambda_vals){
set.seed(123)
# scale the independent variables
X <- scale(X)
# shuffle X and y
ind <- sample(nrow(X), nrow(X), replace = F)
X <- X[ind,]
y <- y[ind]
error <- rep(0, length(lambda_vals))
folds <- as.numeric(cut(1:nrow(X), 5))
for (j in 1: length(lambda_vals)) {
for (i in 1:5){
X_train <- X[folds != i, ]
y_train <- y[folds != i]
X_test <- X[folds == i, ]
y_test <- y[folds == i]
svd_obj <- svd(X_train)
U <- svd_obj$u
V <- svd_obj$v
d <- svd_obj$d
D <- diag(d / (d^2 + lambda_vals[j]))
ridge_beta <- V %*% D %*% t(U) %*% y_train
pred <- as.matrix(X_test) %*% ridge_beta
error[j] <- error[j] + sum(folds==i)/nrow(X) * mean((pred - y_test)^2)
}
}
plot(lambda_vals,error)
lambda_vals[which.min(error)]
}
########## simulation study
### Case 1: higher numerical stability
# simulate a data set
set.seed(123)
n <- 1000
p <- 25
beta <- c(1, rep(0, p - 1))
X <- matrix(rnorm(n * p), ncol = p)
# measure of numerical stability - conditional number
# For OLS
svals <- svd(t(X) %*% X)$d
max(svals) / min(svals) # 1.34 is relatively low
# For Ridge
svals <- svd(t(X) %*% X + 27 * diag(ncol(X)))$d
max(svals) / min(svals)
devtools::build_vignettes()
install.packages("casl")
devtools::uses_testthat()
devtools::uses_testthat()
devtools::use_testthat()
usethis::use_test()
usethis::use_test()
data(iris)
X <- iris[,2:5]
X$Species <- as.numeric(X$Species)
y <- iris[,1]
betahat <- glmnet(X, y, alpha = 0, lambda = 27)
library(glmnet)
betahat <- glmnet(X, y, alpha = 0, lambda = 27)
data(iris)
X <- iris[,2:5]
X$Species <- as.numeric(X$Species)
y <- iris[,1]
betahat <- glmnet(X, y, alpha = 0, lambda = 27)
betahat <- glmnet(as.matrix(X), y, alpha = 0, lambda = 27)
View(betahat)
betahat$beta
fit_ridge <- ridge(as.matrix(X), y, 27)
fit_ridge
betahat <- glmnet(as.matrix(X), y, alpha = 0, lambda = 27)
fit_ridge <- ridge(as.matrix(X), y, 27)
betahat$beta
fit_ridge
install.packages("casl")
casl
library(casl)
install.packages("CASL")
setRepositories()
devtools::install_github("statsmaths/casl")
library(casl)
betahat <- casl::casl_lm_ridge(as.matrix(X), y, lambda = 27)
View(betahat)
fit_ridge
#' @param lambda_vals A vector of possible values of the tuning paramenter \eqn{\lambda} in ridge regression.
#' @return \code{ridge} returns a matrix of regression coefficients with ncol(X) columns
#'   and length(lambda_vals) rows.
#' @examples
#' data(iris)
#' X <- iris[,2:5]
#' X$Species <- as.numeric(X$Species)
#' y <- as.numeric(iris[,1])
#' fit <- ridge(X, y, 0:500)
#' @keywords ridge regression
ridge <- function(X, y, lambda_vals) {
svd_obj <- svd(X)
U <- svd_obj$u
V <- svd_obj$v
d <- svd_obj$d
ridge_beta <- matrix(NA_real_, nrow = length(lambda_vals), ncol = ncol(X))
for (i in 1 : length(lambda_vals)) {
D <- diag(d / (d^2 + lambda_vals[i]))
ridge_beta[i,] <- V %*% D %*% t(U) %*% y
}
ridge_beta
}
betahat <- casl::casl_lm_ridge(as.matrix(X), y, lambda = 27)
fit_ridge <- ridge(as.matrix(X), y, 27)
alpha <- 0.001
X[,1] <- X[,1] * alpha + X[,2] * (1 - alpha)
betahat <- glmnet(as.matrix(X), y, alpha = 0, lambda = 27)
fit_ridge <- ridge(as.matrix(X), y, 27)
betahat <- casl::casl_lm_ridge(as.matrix(X), y, lambda = 94)
fit_ridge <- ridge(as.matrix(X), y, 94)
devtools::test()
context("Estimated coefficients of Ridge regression")
library(casl)
test_that("Your ridge() function works with iris dataset.", {
data(iris)
X <- iris[,2:5]
X$Species <- as.numeric(X$Species)
y <- iris[,1]
betahat <- casl::casl_lm_ridge(as.matrix(X), y, lambda = 27)
fit_ridge <- ridge(as.matrix(X), y, 27)
expect_equivalent(fit_ridge, betahat,
tolerance = 1e-4)
})
devtools::test()
devtools::test()
devtools::test()
install.packages("glmnet")
library(glmnet)
cv_fit <- glmnet::cv.glmnet(x, y, alpha = 0, lambda = lambdas)
cv_fit <- glmnet::cv.glmnet(x, y, alpha = 0, lambda = 1:100)
opt_lambda <- cv_fit$lambda.min
lambdas <- opt_ridge(as.matrix(X), y, 1:100)
lambda <- opt_ridge(as.matrix(X), y, 1:100)
lambda <- opt_ridge(as.matrix(X), y, 1:100)
X <- iris[,2:5]
X$Species <- as.numeric(X$Species)
y <- iris[,1]
cv_fit <- glmnet::cv.glmnet(x, y, alpha = 0, lambda = 1:100)
opt_lambda <- cv_fit$lambda.min
lambda <- opt_ridge(as.matrix(X), y, 1:100)
lambda <- opt_ridge(as.matrix(X), y, 1:100)
data(iris)
X <- iris[,2:5]
X$Species <- as.numeric(X$Species)
y <- iris[,1]
lambda <- opt_ridge(as.matrix(X), y, 1:100)
#' @param lambda_vals A vector of possible values of the tuning paramenter \eqn{\lambda} in ridge regression.
#' @return \code{opt_ridge} returns the optimized value of parameter \eqn{\lambda}.
#'   And a plot of mean squared errors versus \eqn{\lambda} is produced.
#' @examples
#' data(iris)
#' X <- iris[,2:5]
#' X$Species <- as.numeric(X$Species)
#' y <- iris[,1]
#' lambda <- opt_ridge(X, y, 0:500)
#' @keywords ridge regression
opt_ridge <- function (X, y, lambda_vals){
set.seed(123)
# shuffle X and y
ind <- sample(nrow(X), nrow(X), replace = F)
X <- X[ind,]
y <- y[ind]
error <- rep(0, length(lambda_vals))
folds <- as.numeric(cut(1:nrow(X), 5))
for (j in 1: length(lambda_vals)) {
for (i in 1:5){
X_train <- X[folds != i, ]
y_train <- y[folds != i]
X_test <- X[folds == i, ]
y_test <- y[folds == i]
svd_obj <- svd(X_train)
U <- svd_obj$u
V <- svd_obj$v
d <- svd_obj$d
D <- diag(d / (d^2 + lambda_vals[j]))
ridge_beta <- V %*% D %*% t(U) %*% y_train
pred <- as.matrix(X_test) %*% ridge_beta
error[j] <- error[j] + sum(folds==i)/nrow(X) * mean((pred - y_test)^2)
}
}
plot(lambda_vals,error)
lambda_vals[which.min(error)]
}
X <- iris[,2:5]
X$Species <- as.numeric(X$Species)
y <- iris[,1]
lambda <- opt_ridge(X, y, 0:500)
lambda <- opt_ridge(X, y, 1:500)
data(iris)
X <- iris[,2:5]
X$Species <- as.numeric(X$Species)
y <- iris[,1]
lambda <- opt_ridge(X, y, 1:500)
data(iris)
X <- iris[,2:5]
X$Species <- as.numeric(X$Species)
devtools::test()
devtools::use_package("dplyr", "casl")
usethis::use_package("dplyr", "casl")
usethis::use_package("casl", "suggests")
usethis::use_package("glmnet", "suggests")
devtools::build()
install(bis)
install(bis557)
devtools::install(bis557)
devtools::check()
devtools::check()
cv_fit <- glmnet::cv.glmnet(X, y, alpha = 0, lambda = 1:100)
data(iris)
X <- iris[,2:5]
X$Species <- as.numeric(X$Species)
y <- iris[,1]
lambda <- opt_ridge(X, y, 1:500)
cv_fit <- glmnet::cv.glmnet(X, y, alpha = 0, lambda = 1:100)
cv_fit <- glmnet::cv.glmnet(as.matrix(X), y, alpha = 0, lambda = 1:100)
opt_lambda <- cv_fit$lambda.min
lambda <- opt_ridge(X, y, 1:500)
expect_equivalent(lambda, opt_lambda,
tolerance = 1e-4)
devtools::check()
load("/Users/fanny/Desktop/semester 3/Bayesian/homework/Homework_04/HW4.RData")
HW4
load("/Users/fanny/Desktop/semester 3/Bayesian/homework/Homework_04/HW4.RData")
head(HW4)
HW4 <- get(load("/Users/fanny/Desktop/semester 3/Bayesian/homework/Homework_04/HW4.RData"))
devtools::document()
